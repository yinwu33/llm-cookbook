{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prompt Engineering Techniques"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prompt engineering is the process of designing and refining prompts to get the best possible results from a large language model (LLM). In this notebook, we will explore various prompt engineering techniques with code examples using the OpenAI API."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Zero-shot Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zero-shot prompting is the most basic form of prompting. You simply write a prompt that describes the task you want the LLM to perform, without providing any examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import openai\n",
    "import os\n",
    "\n",
    "openai.api_key = os.environ.get(\"OPENAI_API_KEY\")\n",
    "\n",
    "def get_completion(prompt, model=\"gpt-3.5-turbo\"):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=0,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "prompt = \"Translate the following English text to French: 'Hello, how are you?'\"\n",
    "print(get_completion(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Few-shot Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Few-shot prompting involves providing a few examples of the task in the prompt. This can help the LLM to understand the task better and generate more accurate results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Translate the following English text to French:\n",
    "\n",
    "English: 'I love to eat pizza.'\n",
    "French: 'J'adore manger de la pizza.'\n",
    "\n",
    "English: 'The weather is beautiful today.'\n",
    "French: 'Il fait beau aujourd'hui.'\n",
    "\n",
    "English: 'Hello, how are you?'\n",
    "French:\"\"\"\n",
    "print(get_completion(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Chain-of-Thought (CoT) Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chain-of-Thought (CoT) prompting encourages the LLM to generate a series of intermediate reasoning steps before giving the final answer. This is particularly useful for complex reasoning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"Q: Roger has 5 tennis balls. He buys 2 more cans of tennis balls. Each can has 3 tennis balls. How many tennis balls does he have now?\n",
    "A: Roger started with 5 balls. 2 cans of 3 tennis balls each is 6 tennis balls. 5 + 6 = 11. The answer is 11.\n",
    "\n",
    "Q: A juggler can juggle 16 balls. Half of the balls are golf balls, and half of the golf balls are blue. How many blue golf balls are there?\n",
    "A: The juggler has 16 balls in total. Half of the balls are golf balls, so there are 16 / 2 = 8 golf balls. Half of the golf balls are blue, so there are 8 / 2 = 4 blue golf balls. The answer is 4.\n",
    "\n",
    "Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\n",
    "A:\"\"\"\n",
    "print(get_completion(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Self-Consistency"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Self-consistency is a technique that involves generating multiple responses to a prompt and then selecting the most consistent answer. This can help to improve the accuracy of the results, especially for complex reasoning tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_completion_temp(prompt, model=\"gpt-3.5-turbo\", temperature=0.7):\n",
    "    messages = [{\"role\": \"user\", \"content\": prompt}]\n",
    "    response = openai.chat.completions.create(\n",
    "        model=model,\n",
    "        messages=messages,\n",
    "        temperature=temperature,\n",
    "    )\n",
    "    return response.choices[0].message.content\n",
    "\n",
    "prompt = \"\"\"Q: There are 15 trees in the grove. Grove workers will plant trees in the grove today. After they are done, there will be 21 trees. How many trees did the grove workers plant today?\n",
    "A:\"\"\"\n",
    "\n",
    "responses = [get_completion_temp(prompt) for _ in range(3)]\n",
    "for i, response in enumerate(responses):\n",
    "    print(f\"Response {i+1}: {response}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Generated Knowledge Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generated knowledge prompting involves first asking the LLM to generate some knowledge about a topic, and then using that knowledge to answer a question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "knowledge_prompt = \"Generate a short paragraph about the history of the FIFA World Cup.\"\n",
    "knowledge = get_completion(knowledge_prompt)\n",
    "\n",
    "question_prompt = f\"\"\"Based on the following text, answer the question.\n",
    "\n",
    "Text: {knowledge}\n",
    "\n",
    "Question: When was the first FIFA World Cup held?\n",
    "Answer:\"\"\"\n",
    "\n",
    "print(get_completion(question_prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. ReAct (Reason and Act) Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ReAct prompting is a technique that allows the LLM to interact with external tools to gather information. This can be useful for tasks that require up-to-date information or information that is not available in the LLM's training data. A full implementation of ReAct is complex and requires a dedicated library like LangChain or LlamaIndex."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Tree of Thoughts (ToT) Prompting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tree of Thoughts (ToT) prompting is a technique that allows the LLM to explore multiple reasoning paths and self-evaluate them to make deliberate decisions. This can be useful for complex tasks that require a lot of exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Retrieval Augmented Generation (RAG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Retrieval Augmented Generation (RAG) is a technique that combines a retrieval system with a generator model. The retrieval system retrieves relevant documents from a knowledge base, and the generator model uses those documents to generate a response. This is a very powerful technique that is covered in more detail in the `03_basic_rag.ipynb` and `04_advanced_rag.ipynb` notebooks."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
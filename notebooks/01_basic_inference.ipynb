{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3b6ec7d",
   "metadata": {},
   "source": [
    "# Basic LLM Inference\n",
    "\n",
    "This notebook covers the fundamentals of Large Language Model (LLM) inference using APIs. We'll explore different approaches to interact with LLMs and understand key concepts.\n",
    "\n",
    "## What is LLM Inference?\n",
    "\n",
    "LLM inference is the process of generating text using a pre-trained language model. During inference, the model takes an input prompt and generates a response based on patterns learned during training.\n",
    "\n",
    "### Key Concepts:\n",
    "- **Prompt**: The input text that guides the model's response\n",
    "- **Completion**: The text generated by the model\n",
    "- **Temperature**: Controls randomness in generation (0 = deterministic, 1+ = creative)\n",
    "- **Max tokens**: Maximum length of the generated response\n",
    "- **API endpoints**: Different models and their capabilities\n",
    "\n",
    "## Popular LLM API Providers\n",
    "\n",
    "1. **OpenAI**: GPT-3.5, GPT-4, GPT-4 Turbo\n",
    "2. **Anthropic**: Claude 3 (Haiku, Sonnet, Opus)\n",
    "3. **Google**: Gemini Pro\n",
    "4. **Cohere**: Command models\n",
    "5. **Hugging Face**: Various open-source models\n",
    "\n",
    "Let's start with practical examples!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d101f4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ö†Ô∏è  OPENAI_API_KEY not found. Set it with: export OPENAI_API_KEY='your-key'\n",
      "For this tutorial, you can also set it directly:\n",
      "OPENAI_API_KEY = 'your-api-key-here'\n"
     ]
    }
   ],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv(\"../.env\")\n",
    "import os\n",
    "import openai\n",
    "from openai import OpenAI\n",
    "import json\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "# Set up OpenAI API key\n",
    "# You can set this in your environment: export OPENAI_API_KEY=\"your-openai-key\"\n",
    "OPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "if not OPENAI_API_KEY:\n",
    "    print(\"‚ö†Ô∏è  OPENAI_API_KEY not found. Set it with: export OPENAI_API_KEY='your-key'\")\n",
    "    print(\"For this tutorial, you can also set it directly:\")\n",
    "    print(\"OPENAI_API_KEY = 'your-api-key-here'\")\n",
    "else:\n",
    "    # Initialize OpenAI client\n",
    "    client = OpenAI(api_key=OPENAI_API_KEY)\n",
    "    print(\"‚úÖ OpenAI client initialized successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "mq8dvfbf5ck",
   "metadata": {},
   "source": [
    "## 1. Basic OpenAI Chat Completions\n",
    "\n",
    "OpenAI's Chat Completions API is the modern way to interact with GPT models. It supports conversation-style interactions with system, user, and assistant messages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ynkci1nkijm",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ü§ñ GPT Response:\n",
      "‚ùå OpenAI API key not set\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def simple_chat_completion(prompt: str, model: str = \"gpt-3.5-turbo\"):\n",
    "    \"\"\"\n",
    "    Simple chat completion with OpenAI\n",
    "    \n",
    "    Args:\n",
    "        prompt: User message\n",
    "        model: OpenAI model to use\n",
    "    \n",
    "    Returns:\n",
    "        Generated response\n",
    "    \"\"\"\n",
    "    if not OPENAI_API_KEY:\n",
    "        return \"‚ùå OpenAI API key not set\"\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"user\", \"content\": prompt}\n",
    "            ]\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"‚ùå Error: {e}\"\n",
    "\n",
    "# Example 1: Simple question\n",
    "prompt = \"What is machine learning?\"\n",
    "response = simple_chat_completion(prompt)\n",
    "print(\"ü§ñ GPT Response:\")\n",
    "print(response)\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0h8jwcx6h5tl",
   "metadata": {},
   "source": [
    "## 2. Understanding Parameters\n",
    "\n",
    "Let's explore how different parameters affect the model's output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "msosy7ybtv",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üé® Testing Different Temperature Settings:\n",
      "\n",
      "1. Low Temperature (0.2) - More focused and deterministic:\n",
      "‚ùå OpenAI API key not set\n",
      "\n",
      "2. Medium Temperature (0.7) - Balanced creativity:\n",
      "‚ùå OpenAI API key not set\n",
      "\n",
      "3. High Temperature (1.5) - More creative and random:\n",
      "‚ùå OpenAI API key not set\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def chat_with_parameters(prompt: str, temperature: float = 0.7, max_tokens: int = 150, \n",
    "                        top_p: float = 1.0, model: str = \"gpt-3.5-turbo\"):\n",
    "    \"\"\"\n",
    "    Chat completion with customizable parameters\n",
    "    \n",
    "    Args:\n",
    "        prompt: User message\n",
    "        temperature: Controls randomness (0.0 to 2.0)\n",
    "        max_tokens: Maximum tokens to generate\n",
    "        top_p: Nucleus sampling parameter (0.0 to 1.0)\n",
    "        model: OpenAI model to use\n",
    "    \n",
    "    Returns:\n",
    "        Generated response\n",
    "    \"\"\"\n",
    "    if not OPENAI_API_KEY:\n",
    "        return \"‚ùå OpenAI API key not set\"\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            temperature=temperature,\n",
    "            max_tokens=max_tokens,\n",
    "            top_p=top_p\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"‚ùå Error: {e}\"\n",
    "\n",
    "# Test different temperatures\n",
    "prompt = \"Write a creative story about a robot learning to paint.\"\n",
    "\n",
    "print(\"üé® Testing Different Temperature Settings:\")\n",
    "print(\"\\n1. Low Temperature (0.2) - More focused and deterministic:\")\n",
    "response_low = chat_with_parameters(prompt, temperature=0.2, max_tokens=100)\n",
    "print(response_low)\n",
    "\n",
    "print(\"\\n2. Medium Temperature (0.7) - Balanced creativity:\")\n",
    "response_med = chat_with_parameters(prompt, temperature=0.7, max_tokens=100)\n",
    "print(response_med)\n",
    "\n",
    "print(\"\\n3. High Temperature (1.5) - More creative and random:\")\n",
    "response_high = chat_with_parameters(prompt, temperature=1.5, max_tokens=100)\n",
    "print(response_high)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "n1l307yzeh",
   "metadata": {},
   "source": [
    "## 3. System Messages and Role-Based Conversations\n",
    "\n",
    "System messages help set the behavior and context for the AI assistant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "q0tw65mdqnr",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîß Technical Expert Response:\n",
      "‚ùå OpenAI API key not set\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "üë©‚Äçüè´ Beginner-Friendly Response:\n",
      "‚ùå OpenAI API key not set\n",
      "\n",
      "----------------------------------------\n",
      "\n",
      "‚ú® Creative Writer Response:\n",
      "‚ùå OpenAI API key not set\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def chat_with_system_message(user_prompt: str, system_prompt: str, model: str = \"gpt-3.5-turbo\"):\n",
    "    \"\"\"\n",
    "    Chat completion with system message\n",
    "    \n",
    "    Args:\n",
    "        user_prompt: User's message\n",
    "        system_prompt: System message to set AI behavior\n",
    "        model: OpenAI model to use\n",
    "    \n",
    "    Returns:\n",
    "        Generated response\n",
    "    \"\"\"\n",
    "    if not OPENAI_API_KEY:\n",
    "        return \"‚ùå OpenAI API key not set\"\n",
    "    \n",
    "    try:\n",
    "        response = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": system_prompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt}\n",
    "            ]\n",
    "        )\n",
    "        return response.choices[0].message.content\n",
    "    except Exception as e:\n",
    "        return f\"‚ùå Error: {e}\"\n",
    "\n",
    "# Example 1: Technical Expert\n",
    "system_msg_1 = \"You are a senior software engineer with expertise in Python and machine learning. Provide technical, detailed answers.\"\n",
    "user_msg = \"How do I optimize a neural network for better performance?\"\n",
    "\n",
    "response_1 = chat_with_system_message(user_msg, system_msg_1)\n",
    "print(\"üîß Technical Expert Response:\")\n",
    "print(response_1)\n",
    "print(\"\\n\" + \"-\"*40 + \"\\n\")\n",
    "\n",
    "# Example 2: Simple Explainer\n",
    "system_msg_2 = \"You are a friendly teacher explaining complex topics to beginners. Use simple language and analogies.\"\n",
    "response_2 = chat_with_system_message(user_msg, system_msg_2)\n",
    "print(\"üë©‚Äçüè´ Beginner-Friendly Response:\")\n",
    "print(response_2)\n",
    "print(\"\\n\" + \"-\"*40 + \"\\n\")\n",
    "\n",
    "# Example 3: Creative Writer\n",
    "system_msg_3 = \"You are a creative writer who explains everything through storytelling and metaphors.\"\n",
    "response_3 = chat_with_system_message(user_msg, system_msg_3)\n",
    "print(\"‚ú® Creative Writer Response:\")\n",
    "print(response_3)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "um5adog156j",
   "metadata": {},
   "source": [
    "## 4. Multi-turn Conversations\n",
    "\n",
    "Build conversations with multiple exchanges between user and assistant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "29z7ibx4tsw",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üó£Ô∏è Multi-turn Conversation Example:\n",
      "==================================================\n",
      "üë§ User: What is a list comprehension in Python?\n",
      "ü§ñ Assistant: ‚ùå OpenAI API key not set\n",
      "\n",
      "------------------------------\n",
      "\n",
      "üë§ User: Can you show me an example with filtering?\n",
      "ü§ñ Assistant: ‚ùå OpenAI API key not set\n",
      "\n",
      "------------------------------\n",
      "\n",
      "üë§ User: What's the performance difference compared to regular loops?\n",
      "ü§ñ Assistant: ‚ùå OpenAI API key not set\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "class ChatSession:\n",
    "    \"\"\"Simple chat session manager\"\"\"\n",
    "    \n",
    "    def __init__(self, system_message: str = None, model: str = \"gpt-3.5-turbo\"):\n",
    "        self.messages = []\n",
    "        self.model = model\n",
    "        \n",
    "        if system_message:\n",
    "            self.messages.append({\"role\": \"system\", \"content\": system_message})\n",
    "    \n",
    "    def add_user_message(self, content: str):\n",
    "        \"\"\"Add user message to conversation\"\"\"\n",
    "        self.messages.append({\"role\": \"user\", \"content\": content})\n",
    "    \n",
    "    def add_assistant_message(self, content: str):\n",
    "        \"\"\"Add assistant message to conversation\"\"\"\n",
    "        self.messages.append({\"role\": \"assistant\", \"content\": content})\n",
    "    \n",
    "    def get_response(self):\n",
    "        \"\"\"Get response from OpenAI\"\"\"\n",
    "        if not OPENAI_API_KEY:\n",
    "            return \"‚ùå OpenAI API key not set\"\n",
    "        \n",
    "        try:\n",
    "            response = client.chat.completions.create(\n",
    "                model=self.model,\n",
    "                messages=self.messages\n",
    "            )\n",
    "            \n",
    "            assistant_response = response.choices[0].message.content\n",
    "            self.add_assistant_message(assistant_response)\n",
    "            return assistant_response\n",
    "            \n",
    "        except Exception as e:\n",
    "            return f\"‚ùå Error: {e}\"\n",
    "    \n",
    "    def print_conversation(self):\n",
    "        \"\"\"Print the entire conversation\"\"\"\n",
    "        for msg in self.messages:\n",
    "            role = msg[\"role\"].title()\n",
    "            if role == \"System\":\n",
    "                print(f\"üîß {role}: {msg['content']}\")\n",
    "            elif role == \"User\":\n",
    "                print(f\"üë§ {role}: {msg['content']}\")\n",
    "            else:\n",
    "                print(f\"ü§ñ {role}: {msg['content']}\")\n",
    "            print()\n",
    "\n",
    "# Example: Multi-turn conversation about Python\n",
    "chat = ChatSession(system_message=\"You are a helpful Python programming tutor.\")\n",
    "\n",
    "print(\"üó£Ô∏è Multi-turn Conversation Example:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Turn 1\n",
    "chat.add_user_message(\"What is a list comprehension in Python?\")\n",
    "response1 = chat.get_response()\n",
    "print(\"üë§ User: What is a list comprehension in Python?\")\n",
    "print(f\"ü§ñ Assistant: {response1}\")\n",
    "print(\"\\n\" + \"-\"*30 + \"\\n\")\n",
    "\n",
    "# Turn 2\n",
    "chat.add_user_message(\"Can you show me an example with filtering?\")\n",
    "response2 = chat.get_response()\n",
    "print(\"üë§ User: Can you show me an example with filtering?\")\n",
    "print(f\"ü§ñ Assistant: {response2}\")\n",
    "print(\"\\n\" + \"-\"*30 + \"\\n\")\n",
    "\n",
    "# Turn 3\n",
    "chat.add_user_message(\"What's the performance difference compared to regular loops?\")\n",
    "response3 = chat.get_response()\n",
    "print(\"üë§ User: What's the performance difference compared to regular loops?\")\n",
    "print(f\"ü§ñ Assistant: {response3}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "us1hrouk88r",
   "metadata": {},
   "source": [
    "## 5. Function Calling (Tools)\n",
    "\n",
    "OpenAI models can call functions to interact with external systems and APIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "m40o4fiwd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üõ†Ô∏è Function Calling Examples:\n",
      "==================================================\n",
      "üë§ User: What's the area of a circle with radius 5?\n",
      "ü§ñ Assistant: ‚ùå OpenAI API key not set\n",
      "\n",
      "------------------------------\n",
      "\n",
      "üë§ User: What's the weather like in Tokyo?\n",
      "ü§ñ Assistant: ‚ùå OpenAI API key not set\n",
      "\n",
      "------------------------------\n",
      "\n",
      "üë§ User: Calculate the area of a circle with radius 3 and tell me the weather in Paris\n",
      "ü§ñ Assistant: ‚ùå OpenAI API key not set\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "\n",
    "# Define some example functions\n",
    "def calculate_area_circle(radius: float) -> float:\n",
    "    \"\"\"Calculate the area of a circle\"\"\"\n",
    "    return math.pi * radius ** 2\n",
    "\n",
    "def get_weather_info(city: str) -> str:\n",
    "    \"\"\"Mock weather function (in real app, this would call a weather API)\"\"\"\n",
    "    weather_data = {\n",
    "        \"New York\": \"Sunny, 22¬∞C\",\n",
    "        \"London\": \"Cloudy, 15¬∞C\", \n",
    "        \"Tokyo\": \"Rainy, 18¬∞C\",\n",
    "        \"Paris\": \"Partly cloudy, 19¬∞C\"\n",
    "    }\n",
    "    return weather_data.get(city, f\"Weather data not available for {city}\")\n",
    "\n",
    "# Define function schemas for OpenAI\n",
    "functions = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"calculate_area_circle\",\n",
    "            \"description\": \"Calculate the area of a circle given its radius\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"radius\": {\n",
    "                        \"type\": \"number\",\n",
    "                        \"description\": \"The radius of the circle\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"radius\"]\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"type\": \"function\", \n",
    "        \"function\": {\n",
    "            \"name\": \"get_weather_info\",\n",
    "            \"description\": \"Get weather information for a city\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": {\n",
    "                    \"city\": {\n",
    "                        \"type\": \"string\",\n",
    "                        \"description\": \"The name of the city\"\n",
    "                    }\n",
    "                },\n",
    "                \"required\": [\"city\"]\n",
    "            }\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "def chat_with_functions(user_message: str):\n",
    "    \"\"\"Chat with function calling capability\"\"\"\n",
    "    if not OPENAI_API_KEY:\n",
    "        return \"‚ùå OpenAI API key not set\"\n",
    "    \n",
    "    try:\n",
    "        # Initial request\n",
    "        response = client.chat.completions.create(\n",
    "            model=\"gpt-3.5-turbo\",\n",
    "            messages=[{\"role\": \"user\", \"content\": user_message}],\n",
    "            tools=functions,\n",
    "            tool_choice=\"auto\"\n",
    "        )\n",
    "        \n",
    "        message = response.choices[0].message\n",
    "        \n",
    "        # Check if the model wants to call a function\n",
    "        if message.tool_calls:\n",
    "            print(\"üîß Function calls detected:\")\n",
    "            \n",
    "            # Process each function call\n",
    "            messages = [{\"role\": \"user\", \"content\": user_message}, message]\n",
    "            \n",
    "            for tool_call in message.tool_calls:\n",
    "                function_name = tool_call.function.name\n",
    "                function_args = json.loads(tool_call.function.arguments)\n",
    "                \n",
    "                print(f\"   Calling {function_name} with args: {function_args}\")\n",
    "                \n",
    "                # Call the actual function\n",
    "                if function_name == \"calculate_area_circle\":\n",
    "                    result = calculate_area_circle(**function_args)\n",
    "                elif function_name == \"get_weather_info\":\n",
    "                    result = get_weather_info(**function_args)\n",
    "                else:\n",
    "                    result = \"Unknown function\"\n",
    "                \n",
    "                # Add function result to messages\n",
    "                messages.append({\n",
    "                    \"tool_call_id\": tool_call.id,\n",
    "                    \"role\": \"tool\",\n",
    "                    \"name\": function_name,\n",
    "                    \"content\": str(result)\n",
    "                })\n",
    "            \n",
    "            # Get final response with function results\n",
    "            final_response = client.chat.completions.create(\n",
    "                model=\"gpt-3.5-turbo\",\n",
    "                messages=messages\n",
    "            )\n",
    "            \n",
    "            return final_response.choices[0].message.content\n",
    "        else:\n",
    "            return message.content\n",
    "            \n",
    "    except Exception as e:\n",
    "        return f\"‚ùå Error: {e}\"\n",
    "\n",
    "# Test function calling\n",
    "print(\"üõ†Ô∏è Function Calling Examples:\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "# Example 1: Math calculation\n",
    "query1 = \"What's the area of a circle with radius 5?\"\n",
    "response1 = chat_with_functions(query1)\n",
    "print(f\"üë§ User: {query1}\")\n",
    "print(f\"ü§ñ Assistant: {response1}\")\n",
    "print(\"\\n\" + \"-\"*30 + \"\\n\")\n",
    "\n",
    "# Example 2: Weather query\n",
    "query2 = \"What's the weather like in Tokyo?\"\n",
    "response2 = chat_with_functions(query2)\n",
    "print(f\"üë§ User: {query2}\")\n",
    "print(f\"ü§ñ Assistant: {response2}\")\n",
    "print(\"\\n\" + \"-\"*30 + \"\\n\")\n",
    "\n",
    "# Example 3: Combined query\n",
    "query3 = \"Calculate the area of a circle with radius 3 and tell me the weather in Paris\"\n",
    "response3 = chat_with_functions(query3)\n",
    "print(f\"üë§ User: {query3}\")\n",
    "print(f\"ü§ñ Assistant: {response3}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "qa9gv4s8bx",
   "metadata": {},
   "source": [
    "## 6. Streaming Responses\n",
    "\n",
    "For longer responses, streaming allows you to see the output as it's generated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ulb87j2gf6r",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚ùå OpenAI API key not set\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def stream_response(prompt: str, model: str = \"gpt-3.5-turbo\"):\n",
    "    \"\"\"Stream response from OpenAI\"\"\"\n",
    "    if not OPENAI_API_KEY:\n",
    "        print(\"‚ùå OpenAI API key not set\")\n",
    "        return\n",
    "    \n",
    "    try:\n",
    "        print(\"ü§ñ Streaming response:\")\n",
    "        print(\"-\" * 40)\n",
    "        \n",
    "        stream = client.chat.completions.create(\n",
    "            model=model,\n",
    "            messages=[{\"role\": \"user\", \"content\": prompt}],\n",
    "            stream=True\n",
    "        )\n",
    "        \n",
    "        collected_content = \"\"\n",
    "        for chunk in stream:\n",
    "            if chunk.choices[0].delta.content is not None:\n",
    "                content = chunk.choices[0].delta.content\n",
    "                print(content, end=\"\", flush=True)\n",
    "                collected_content += content\n",
    "                time.sleep(0.02)  # Small delay to simulate typing effect\n",
    "        \n",
    "        print(\"\\n\" + \"-\" * 40)\n",
    "        return collected_content\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error: {e}\")\n",
    "\n",
    "# Example: Stream a longer response\n",
    "prompt = \"Write a detailed explanation of how neural networks learn, including backpropagation.\"\n",
    "response = stream_response(prompt)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6700o13pq",
   "metadata": {},
   "source": [
    "## 7. Best Practices and Common Patterns\n",
    "\n",
    "Key takeaways for effective LLM inference:\n",
    "\n",
    "### üéØ Parameter Guidelines:\n",
    "- **Temperature 0.0-0.3**: Factual, consistent responses\n",
    "- **Temperature 0.7-1.0**: Creative, varied responses  \n",
    "- **Temperature 1.5+**: Highly creative but potentially inconsistent\n",
    "\n",
    "### üèóÔ∏è System Message Patterns:\n",
    "- Define role and expertise clearly\n",
    "- Set output format expectations\n",
    "- Provide context and constraints\n",
    "\n",
    "### üí° Prompt Design Tips:\n",
    "- Be specific and clear\n",
    "- Use examples when possible\n",
    "- Break complex tasks into steps\n",
    "- Set clear expectations\n",
    "\n",
    "### üîß Error Handling:\n",
    "- Always handle API key missing\n",
    "- Catch and handle HTTP errors\n",
    "- Implement retry logic for production\n",
    "- Monitor token usage and costs\n",
    "\n",
    "### üöÄ Performance Tips:\n",
    "- Use appropriate model for task complexity\n",
    "- Implement streaming for long responses\n",
    "- Cache responses when possible\n",
    "- Batch requests when applicable\n",
    "\n",
    "This completes the basic inference tutorial! You now understand the fundamentals of working with OpenAI's API for text generation."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm-cookbook",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

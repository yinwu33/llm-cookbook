{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1f4f4f1",
   "metadata": {},
   "source": [
    "This Notebook is based on [Hugging Face Blog](https://huggingface.co/blog/dvgodoy/fine-tuning-llm-hugging-face), showing how to finetune a quanted LLM with Lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4c4fc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from datasets import load_dataset\n",
    "from peft import get_peft_model, LoraConfig, prepare_model_for_kbit_training\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, BitsAndBytesConfig\n",
    "from trl import SFTConfig, SFTTrainer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb517ab7",
   "metadata": {},
   "source": [
    "* peft: pretrained efficient fine-tuning\n",
    "* trl: transformer reinforcement learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a764029",
   "metadata": {},
   "source": [
    "# Model Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "44027496",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:02<00:00,  1.43s/it]\n"
     ]
    }
   ],
   "source": [
    "bnb_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True,\n",
    "    bnb_4bit_compute_dtype=torch.bfloat16,\n",
    ")\n",
    "\n",
    "# use qwen model\n",
    "repo_id = \"Qwen/Qwen2.5-3B-Instruct\"\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    repo_id,\n",
    "    quantization_config=bnb_config,\n",
    "    device_map=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "ddb0a1d4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2010.088704 MB\n"
     ]
    }
   ],
   "source": [
    "print(f\"{model.get_memory_footprint()/1e6} MB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81152f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Qwen2ForCausalLM(\n",
       "  (model): Qwen2Model(\n",
       "    (embed_tokens): Embedding(151936, 2048)\n",
       "    (layers): ModuleList(\n",
       "      (0-35): 36 x Qwen2DecoderLayer(\n",
       "        (self_attn): Qwen2SdpaAttention(\n",
       "          (q_proj): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
       "          (k_proj): Linear4bit(in_features=2048, out_features=256, bias=True)\n",
       "          (v_proj): Linear4bit(in_features=2048, out_features=256, bias=True)\n",
       "          (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "          (rotary_emb): Qwen2RotaryEmbedding()\n",
       "        )\n",
       "        (mlp): Qwen2MLP(\n",
       "          (gate_proj): Linear4bit(in_features=2048, out_features=11008, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=2048, out_features=11008, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=11008, out_features=2048, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "        (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "    (rotary_emb): Qwen2RotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "4a7a75c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "PeftModelForCausalLM(\n",
       "  (base_model): LoraModel(\n",
       "    (model): Qwen2ForCausalLM(\n",
       "      (model): Qwen2Model(\n",
       "        (embed_tokens): Embedding(151936, 2048)\n",
       "        (layers): ModuleList(\n",
       "          (0-35): 36 x Qwen2DecoderLayer(\n",
       "            (self_attn): Qwen2SdpaAttention(\n",
       "              (q_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=2048, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=2048, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (k_proj): Linear4bit(in_features=2048, out_features=256, bias=True)\n",
       "              (v_proj): lora.Linear4bit(\n",
       "                (base_layer): Linear4bit(in_features=2048, out_features=256, bias=True)\n",
       "                (lora_dropout): ModuleDict(\n",
       "                  (default): Dropout(p=0.05, inplace=False)\n",
       "                )\n",
       "                (lora_A): ModuleDict(\n",
       "                  (default): Linear(in_features=2048, out_features=8, bias=False)\n",
       "                )\n",
       "                (lora_B): ModuleDict(\n",
       "                  (default): Linear(in_features=8, out_features=256, bias=False)\n",
       "                )\n",
       "                (lora_embedding_A): ParameterDict()\n",
       "                (lora_embedding_B): ParameterDict()\n",
       "                (lora_magnitude_vector): ModuleDict()\n",
       "              )\n",
       "              (o_proj): Linear4bit(in_features=2048, out_features=2048, bias=False)\n",
       "              (rotary_emb): Qwen2RotaryEmbedding()\n",
       "            )\n",
       "            (mlp): Qwen2MLP(\n",
       "              (gate_proj): Linear4bit(in_features=2048, out_features=11008, bias=False)\n",
       "              (up_proj): Linear4bit(in_features=2048, out_features=11008, bias=False)\n",
       "              (down_proj): Linear4bit(in_features=11008, out_features=2048, bias=False)\n",
       "              (act_fn): SiLU()\n",
       "            )\n",
       "            (input_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "            (post_attention_layernorm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "          )\n",
       "        )\n",
       "        (norm): Qwen2RMSNorm((2048,), eps=1e-06)\n",
       "        (rotary_emb): Qwen2RotaryEmbedding()\n",
       "      )\n",
       "      (lm_head): Linear(in_features=2048, out_features=151936, bias=False)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# prepare for lora training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "lora_config = LoraConfig(\n",
    "    r=8,  # rank\n",
    "    lora_alpha=16,\n",
    "    target_modules=[\"q_proj\", \"v_proj\"],\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\",\n",
    ")\n",
    "model = get_peft_model(model, lora_config)\n",
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9197c43e",
   "metadata": {},
   "source": [
    "prepare_model_for_kbit_training(model)\n",
    "\n",
    "* 功能：让一个已经量化（如 4bit/8bit）加载的模型适配训练。\n",
    "* 主要做的事：\n",
    "    * 冻结除 LoRA 层以外的所有权重（避免反向传播更新它们）\n",
    "    * 把 LayerNorm 类层的参数转成 FP32（保持数值稳定）\n",
    "    * 确保梯度检查点和混合精度设置能正常工作\n",
    "    * 可选地启用 gradient checkpointing 来节省显存\n",
    "* 为什么需要：\n",
    "    * 量化权重是 int4/int8，不支持反向梯度更新\n",
    "    * 微调时只训练 LoRA 参数，不改动量化权重\n",
    "    * 有些操作需要高精度，否则会梯度爆炸或 NaN\n",
    "\n",
    "LoraConfig(...)\n",
    "这是 LoRA 的配置对象，定义你要插入的 LoRA 层的形状、位置和训练超参。\n",
    "\n",
    "* r=8\n",
    "    * LoRA 的秩（rank），相当于低秩矩阵分解里的中间维度\n",
    "    * 越大，可训练参数越多，表示能力越强，但显存和计算开销也更大\n",
    "* lora_alpha=16\n",
    "    * LoRA 的缩放系数，控制更新幅度（类似学习率放大器）\n",
    "    * 有效权重更新公式：dW = a/r *A*B\n",
    "* target_modules=[\"q_proj\", \"v_proj\"]\n",
    "    * 只在注意力层的 Query 投影（q_proj）和 Value 投影（v_proj）加 LoRA\n",
    "    * 这是 QLoRA 论文常用配置，因为对模型性能影响大、显存开销小\n",
    "* lora_dropout=0.05\n",
    "    * 对 LoRA 输入加 dropout，防止过拟合\n",
    "* bias=\"none\"\n",
    "    * 不训练 bias 参数（减少开销）\n",
    "* task_type=\"CAUSAL_LM\"\n",
    "    * 告诉 PEFT 这是因果语言建模任务（Causal Language Modeling）\n",
    "\n",
    "model = get_peft_model(model, lora_config)\n",
    "* 作用：根据 lora_config，在 model 指定的模块里插入 LoRA 层\n",
    "* 执行结果：\n",
    "    * q_proj 和 v_proj 原来的 Linear4bit 会被包装成 LoRA 版本\n",
    "    * 训练时只更新这些 LoRA 参数（A、B 矩阵），其余部分冻结\n",
    "* 优点:\n",
    "    * 大部分参数保持量化（显存低）\n",
    "    * 可训练参数量只有原模型的几百万到几千万分之一，训练快"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "5c864984",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,843,200 || all params: 3,087,781,888 || trainable%: 0.0597\n"
     ]
    }
   ],
   "source": [
    "model.print_trainable_parameters()  # 查看确实只有 LoRA 在训"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "20cb1b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2640.274688\n"
     ]
    }
   ],
   "source": [
    "print(model.get_memory_footprint()/1e6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b96b01df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainable parameters:      1.84M\n",
      "Total parameters:          3087.78M\n",
      "% of trainable parameters: 0.06%\n"
     ]
    }
   ],
   "source": [
    "train_p, tot_p = model.get_nb_trainable_parameters()\n",
    "print(f'Trainable parameters:      {train_p/1e6:.2f}M')\n",
    "print(f'Total parameters:          {tot_p/1e6:.2f}M')\n",
    "print(f'% of trainable parameters: {100*train_p/tot_p:.2f}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ddd158d",
   "metadata": {},
   "source": [
    "# Dataset Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02b48135",
   "metadata": {},
   "source": [
    "## 为什么这两个结果不一样？\n",
    "\n",
    "### 1. `model.get_memory_footprint()`\n",
    "- **作用**：估算 **整个模型当前占用的显存/内存大小**（以字节为单位）\n",
    "- 计算方式：\n",
    "  - 遍历所有参数张量（`model.parameters()`）和缓冲区（buffers）\n",
    "  - 根据张量的 `numel()` × `element_size()` 计算字节数\n",
    "  - **会考虑数据类型**（FP32=4B，BF16=2B，int4≈0.5B）\n",
    "- 特点：\n",
    "  - 这是“运行时内存占用”，和参数总个数无关，**和量化精度直接相关**\n",
    "  - 4bit 量化模型会比 FP16/FP32 模型占用显存小很多\n",
    "\n",
    "---\n",
    "\n",
    "### 2. `model.get_nb_trainable_parameters()`\n",
    "- **作用**：统计**可训练参数数量**与**总参数数量**（按个数，不按内存）\n",
    "- 计算方式：\n",
    "  - 遍历所有 `model.parameters()`  \n",
    "    - 总参数数 = 所有 `numel()` 相加\n",
    "    - 可训练参数数 = `requires_grad=True` 的 `numel()` 相加\n",
    "- 特点：\n",
    "  - 这是**参数个数统计**，**不考虑数据类型**  \n",
    "  - 不管是 int4、FP16 还是 FP32，一个参数就是“1 个参数”\n",
    "\n",
    "---\n",
    "\n",
    "### 3. 为什么结果不同？\n",
    "- **统计维度不同**\n",
    "  - `get_memory_footprint()` → 按**字节数**计算（受 dtype 影响）\n",
    "  - `get_nb_trainable_parameters()` → 按**参数个数**计算（与 dtype 无关）\n",
    "- **量化的影响**\n",
    "  - 量化后参数个数不变（所以 `total parameters` 一样）\n",
    "  - 但存储精度下降（比如 FP16 → int4），显存占用大幅下降\n",
    "- **LoRA 的影响**\n",
    "  - 大部分基座参数被冻结（`requires_grad=False`），`trainable parameters` 只有 LoRA 层那部分\n",
    "  - 基座参数虽然不训练，但仍然会占内存（所以 footprint 里有它们）\n",
    "\n",
    "---\n",
    "\n",
    "### 4. 举例\n",
    "假设：\n",
    "- 原模型：100M 参数，FP16（2 字节/参数） → 占 200MB\n",
    "- LoRA：只训练 2M 参数（FP32）\n",
    "- 量化基座：98M 参数 int4（0.5 字节/参数）\n",
    "\n",
    "结果：\n",
    "- **`get_nb_trainable_parameters()`**  \n",
    "  - Trainable = 2M  \n",
    "  - Total = 100M  \n",
    "  - 比例 = 2%\n",
    "- **`get_memory_footprint()`**  \n",
    "  - Trainable：2M × 4B ≈ 8MB  \n",
    "  - 冻结基座：98M × 0.5B ≈ 49MB  \n",
    "  - 总占用 ≈ 57MB（远小于原 FP16 模型的 200MB）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c37a32e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['sentence', 'translation', 'translation_extra'],\n",
       "    num_rows: 720\n",
       "})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset(\"dvgodoy/yoda_sentences\", split=\"train\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "03da4c15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sentence': 'The birch canoe slid on the smooth planks.',\n",
       " 'translation': 'On the smooth planks, the birch canoe slid.',\n",
       " 'translation_extra': 'On the smooth planks, the birch canoe slid. Yes, hrrrm.'}"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "ed56c8ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MemoryMappedTable\n",
       "messages: list<item: struct<content: string, role: string>>\n",
       "  child 0, item: struct<content: string, role: string>\n",
       "      child 0, content: string\n",
       "      child 1, role: string\n",
       "----\n",
       "messages: [[    -- is_valid: all not null\n",
       "    -- child 0 type: string\n",
       "[\"The birch canoe slid on the smooth planks.\",\"On the smooth planks, the birch canoe slid. Yes, hrrrm.\"]\n",
       "    -- child 1 type: string\n",
       "[\"user\",\"assistant\"],    -- is_valid: all not null\n",
       "    -- child 0 type: string\n",
       "[\"Glue the sheet to the dark blue background.\",\"Glue the sheet to the dark blue background, you must.\"]\n",
       "    -- child 1 type: string\n",
       "[\"user\",\"assistant\"],...,    -- is_valid: all not null\n",
       "    -- child 0 type: string\n",
       "[\"She called his name many times.\",\"Hrrmmm. His name many times, she called. Hrmmm.\"]\n",
       "    -- child 1 type: string\n",
       "[\"user\",\"assistant\"],    -- is_valid: all not null\n",
       "    -- child 0 type: string\n",
       "[\"When you hear the bell, come quickly.\",\"Hrrmmm. When the bell you hear, come quickly, you must.\"]\n",
       "    -- child 1 type: string\n",
       "[\"user\",\"assistant\"]]]"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "861d5256",
   "metadata": {},
   "source": [
    "SFTTrainer 支持 Conversational format和Instruction Format e.g. \n",
    "\n",
    "```text\n",
    "Conversational\n",
    "{\"messages\":[\n",
    "  {\"role\": \"system\", \"content\": \"<general directives>\"},\n",
    "  {\"role\": \"user\", \"content\": \"<prompt text>\"},\n",
    "  {\"role\": \"assistant\", \"content\": \"<ideal generated text>\"}\n",
    "]}\n",
    "\n",
    "Instruction\n",
    "{\"prompt\": \"<prompt text>\",\n",
    "\"completion\": \"<ideal generated text>\"}\n",
    "```\n",
    "\n",
    "但是为了更好的适配性，conversational类型是更推荐的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "30fcc248",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'prompt': 'The birch canoe slid on the smooth planks.',\n",
       " 'completion': 'On the smooth planks, the birch canoe slid. Yes, hrrrm.'}"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = dataset.rename_column(\"sentence\", \"prompt\")\n",
    "dataset = dataset.rename_column(\"translation_extra\", \"completion\")\n",
    "dataset = dataset.remove_columns([\"translation\"])\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "d1c1ff19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'content': 'The birch canoe slid on the smooth planks.', 'role': 'user'},\n",
       " {'content': 'On the smooth planks, the birch canoe slid. Yes, hrrrm.',\n",
       "  'role': 'assistant'}]"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Adapted from trl.extras.dataset_formatting.instructions_formatting_function\n",
    "# Converts dataset from prompt/completion format (not supported anymore)\n",
    "# to the conversational format\n",
    "def format_dataset(examples):\n",
    "    if isinstance(examples[\"prompt\"], list):\n",
    "        output_texts = []\n",
    "        for i in range(len(examples[\"prompt\"])):\n",
    "            converted_sample = [\n",
    "                {\"role\": \"user\", \"content\": examples[\"prompt\"][i]},\n",
    "                {\"role\": \"assistant\", \"content\": examples[\"completion\"][i]},\n",
    "            ]\n",
    "            output_texts.append(converted_sample)\n",
    "        return {'messages': output_texts}\n",
    "    else:\n",
    "        converted_sample = [\n",
    "            {\"role\": \"user\", \"content\": examples[\"prompt\"]},\n",
    "            {\"role\": \"assistant\", \"content\": examples[\"completion\"]},\n",
    "        ]\n",
    "        return {'messages': converted_sample}\n",
    "\n",
    "dataset = dataset.map(format_dataset).remove_columns(['prompt', 'completion'])\n",
    "dataset[0]['messages']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff1dc89f",
   "metadata": {},
   "source": [
    "# Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "c225980e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Qwen/Qwen2.5-3B-Instruct\n",
      "Qwen2TokenizerFast(name_or_path='Qwen/Qwen2.5-3B-Instruct', vocab_size=151643, model_max_length=131072, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'eos_token': '<|im_end|>', 'pad_token': '<|endoftext|>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>', '<|object_ref_start|>', '<|object_ref_end|>', '<|box_start|>', '<|box_end|>', '<|quad_start|>', '<|quad_end|>', '<|vision_start|>', '<|vision_end|>', '<|vision_pad|>', '<|image_pad|>', '<|video_pad|>']}, clean_up_tokenization_spaces=False),  added_tokens_decoder={\n",
      "\t151643: AddedToken(\"<|endoftext|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151644: AddedToken(\"<|im_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151645: AddedToken(\"<|im_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151646: AddedToken(\"<|object_ref_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151647: AddedToken(\"<|object_ref_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151648: AddedToken(\"<|box_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151649: AddedToken(\"<|box_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151650: AddedToken(\"<|quad_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151651: AddedToken(\"<|quad_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151652: AddedToken(\"<|vision_start|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151653: AddedToken(\"<|vision_end|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151654: AddedToken(\"<|vision_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151655: AddedToken(\"<|image_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151656: AddedToken(\"<|video_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=True),\n",
      "\t151657: AddedToken(\"<tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151658: AddedToken(\"</tool_call>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151659: AddedToken(\"<|fim_prefix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151660: AddedToken(\"<|fim_middle|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151661: AddedToken(\"<|fim_suffix|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151662: AddedToken(\"<|fim_pad|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151663: AddedToken(\"<|repo_name|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "\t151664: AddedToken(\"<|file_sep|>\", rstrip=False, lstrip=False, single_word=False, normalized=False, special=False),\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(repo_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(repo_id)\n",
    "print(tokenizer)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8114586c",
   "metadata": {},
   "source": [
    "Tokenizer负责把文字翻译成token ID, e.g. \"Hello World\" -> [10123, 12341, 12321]。\n",
    "\n",
    "一个经典的tokenizer可以有以下几个工作步骤\n",
    "\n",
    "1. Preprocessing\n",
    "\n",
    "    * 统一大小写（或者不统一）\n",
    "    * 处理Unicode正规化\n",
    "    * 特殊字符处理 （中文英文之间加入空格）\n",
    "2. 特殊标记插入\n",
    "    * <bos>: begin of sentence\n",
    "    * <eos>: end of sentence\n",
    "    * <pad>: padding 用于补齐长度\n",
    "    * <unk>: unknown token\n",
    "    * 对话标识符: <|im_start|> <|im_end|>\n",
    "    * 注意，这些标识符也有对应的token ID, e.g. <|im_start|> = 100264\n",
    "3. tokenization 分词\n",
    "    * BPE (Byte Pair Encoding)，用尽量长的token单元代替\n",
    "    * SentencePiece / WordPiece\n",
    "    * e.g., \"playing\" = [\"play\", \"ing\"] = [1234, 567]\n",
    "\n",
    "\n",
    "模型最后输出也是token ID序列，然后进行词汇表反查，拼接成句子。\n",
    "\n",
    "tokenizer输出的是N个token，之后交给embedding进行查表，embedding本身是一个mapping，用来将token ID 转换成一个个feature vector\n",
    "* <sentence> -> Tokenizer -> <tokens: [N,]>\n",
    "* <tokens> -> Embedding -> <embeddings: [N, C]>\n",
    "* <embeddings> -> Transformers -> Linear -> <logits: [N, V]>: V is the vocabular size, use max prob to get token (like classification task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "f8ca6025",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'{%- if tools %}\\n    {{- \\'<|im_start|>system\\\\n\\' }}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- messages[0][\\'content\\'] }}\\n    {%- else %}\\n        {{- \\'You are Qwen, created by Alibaba Cloud. You are a helpful assistant.\\' }}\\n    {%- endif %}\\n    {{- \"\\\\n\\\\n# Tools\\\\n\\\\nYou may call one or more functions to assist with the user query.\\\\n\\\\nYou are provided with function signatures within <tools></tools> XML tags:\\\\n<tools>\" }}\\n    {%- for tool in tools %}\\n        {{- \"\\\\n\" }}\\n        {{- tool | tojson }}\\n    {%- endfor %}\\n    {{- \"\\\\n</tools>\\\\n\\\\nFor each function call, return a json object with function name and arguments within <tool_call></tool_call> XML tags:\\\\n<tool_call>\\\\n{\\\\\"name\\\\\": <function-name>, \\\\\"arguments\\\\\": <args-json-object>}\\\\n</tool_call><|im_end|>\\\\n\" }}\\n{%- else %}\\n    {%- if messages[0][\\'role\\'] == \\'system\\' %}\\n        {{- \\'<|im_start|>system\\\\n\\' + messages[0][\\'content\\'] + \\'<|im_end|>\\\\n\\' }}\\n    {%- else %}\\n        {{- \\'<|im_start|>system\\\\nYou are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\\\\n\\' }}\\n    {%- endif %}\\n{%- endif %}\\n{%- for message in messages %}\\n    {%- if (message.role == \"user\") or (message.role == \"system\" and not loop.first) or (message.role == \"assistant\" and not message.tool_calls) %}\\n        {{- \\'<|im_start|>\\' + message.role + \\'\\\\n\\' + message.content + \\'<|im_end|>\\' + \\'\\\\n\\' }}\\n    {%- elif message.role == \"assistant\" %}\\n        {{- \\'<|im_start|>\\' + message.role }}\\n        {%- if message.content %}\\n            {{- \\'\\\\n\\' + message.content }}\\n        {%- endif %}\\n        {%- for tool_call in message.tool_calls %}\\n            {%- if tool_call.function is defined %}\\n                {%- set tool_call = tool_call.function %}\\n            {%- endif %}\\n            {{- \\'\\\\n<tool_call>\\\\n{\"name\": \"\\' }}\\n            {{- tool_call.name }}\\n            {{- \\'\", \"arguments\": \\' }}\\n            {{- tool_call.arguments | tojson }}\\n            {{- \\'}\\\\n</tool_call>\\' }}\\n        {%- endfor %}\\n        {{- \\'<|im_end|>\\\\n\\' }}\\n    {%- elif message.role == \"tool\" %}\\n        {%- if (loop.index0 == 0) or (messages[loop.index0 - 1].role != \"tool\") %}\\n            {{- \\'<|im_start|>user\\' }}\\n        {%- endif %}\\n        {{- \\'\\\\n<tool_response>\\\\n\\' }}\\n        {{- message.content }}\\n        {{- \\'\\\\n</tool_response>\\' }}\\n        {%- if loop.last or (messages[loop.index0 + 1].role != \"tool\") %}\\n            {{- \\'<|im_end|>\\\\n\\' }}\\n        {%- endif %}\\n    {%- endif %}\\n{%- endfor %}\\n{%- if add_generation_prompt %}\\n    {{- \\'<|im_start|>assistant\\\\n\\' }}\\n{%- endif %}\\n'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.chat_template\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "dc9f12ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'content': 'The birch canoe slid on the smooth planks.', 'role': 'user'}, {'content': 'On the smooth planks, the birch canoe slid. Yes, hrrrm.', 'role': 'assistant'}]\n",
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "The birch canoe slid on the smooth planks.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "On the smooth planks, the birch canoe slid. Yes, hrrrm.<|im_end|>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "messages = dataset[0]['messages']\n",
    "print(messages)\n",
    "print(tokenizer.apply_chat_template(messages, tokenize=False))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eda20f34",
   "metadata": {},
   "source": [
    "# Finetune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "7f760239",
   "metadata": {},
   "outputs": [],
   "source": [
    "sft_config = SFTConfig(\n",
    "    ## GROUP 1: Memory usage\n",
    "    # These arguments will squeeze the most out of your GPU's RAM\n",
    "    # Checkpointing\n",
    "    gradient_checkpointing=True,    # this saves a LOT of memory\n",
    "    # Set this to avoid exceptions in newer versions of PyTorch\n",
    "    gradient_checkpointing_kwargs={'use_reentrant': False}, \n",
    "    # Gradient Accumulation / Batch size\n",
    "    # Actual batch (for updating) is same (1x) as micro-batch size\n",
    "    gradient_accumulation_steps=1,  \n",
    "    # The initial (micro) batch size to start off with\n",
    "    per_device_train_batch_size=16, \n",
    "    # If batch size would cause OOM, halves its size until it works\n",
    "    auto_find_batch_size=True,\n",
    "\n",
    "    ## GROUP 2: Dataset-related\n",
    "    max_seq_length=64,\n",
    "    # Dataset\n",
    "    # packing a dataset means no padding is needed\n",
    "    packing=True,\n",
    "\n",
    "    ## GROUP 3: These are typical training parameters\n",
    "    num_train_epochs=10,\n",
    "    learning_rate=3e-4,\n",
    "    # Optimizer\n",
    "    # 8-bit Adam optimizer - doesn't help much if you're using LoRA!\n",
    "    optim='paged_adamw_8bit',       \n",
    "    \n",
    "    ## GROUP 4: Logging parameters\n",
    "    logging_steps=10,\n",
    "    logging_dir='./logs',\n",
    "    output_dir='./qwen-2_5-mini-yoda-adapter',\n",
    "    report_to='none'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "f8840133",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    args=sft_config,\n",
    "    train_dataset=dataset,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "521e35d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([   198,   2610,    525,   1207,  16948,     11,   3465,    553,  54364,\n",
       "          14817,     13,   1446,    525,    264,  10950,  17847,     13, 151645,\n",
       "            198, 151644,    872,    198,  58400,    383,    323,  11967,    304,\n",
       "            279,   7010,   6176,  16359,     13, 151645,    198, 151644,  77091,\n",
       "            198,    641,    279,   7010,   6176,  16359,     11,  15743,    383,\n",
       "            323,  11967,     11,    498,   1969,     13, 151645,    198, 151645,\n",
       "         151644,   8948,    198,   2610,    525,   1207,  16948,     11,   3465,\n",
       "            553], device='cuda:0'),\n",
       " tensor([   198,   2610,    525,   1207,  16948,     11,   3465,    553,  54364,\n",
       "          14817,     13,   1446,    525,    264,  10950,  17847,     13, 151645,\n",
       "            198, 151644,    872,    198,  58400,    383,    323,  11967,    304,\n",
       "            279,   7010,   6176,  16359,     13, 151645,    198, 151644,  77091,\n",
       "            198,    641,    279,   7010,   6176,  16359,     11,  15743,    383,\n",
       "            323,  11967,     11,    498,   1969,     13, 151645,    198, 151645,\n",
       "         151644,   8948,    198,   2610,    525,   1207,  16948,     11,   3465,\n",
       "            553], device='cuda:0'))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dl = trainer.get_train_dataloader()\n",
    "batch = next(iter(dl))\n",
    "batch['input_ids'][0], batch['labels'][0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "cd06012c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`...\n",
      "/home/tjhu78u/miniconda3/envs/llm/lib/python3.10/site-packages/torch/utils/checkpoint.py:1399: FutureWarning: `torch.cpu.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cpu', args...)` instead.\n",
      "  with device_autocast_ctx, torch.cpu.amp.autocast(**cpu_autocast_kwargs), recompute_context:  # type: ignore[attr-defined]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='390' max='390' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [390/390 07:26, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.804000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.930300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>30</td>\n",
       "      <td>2.190400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>40</td>\n",
       "      <td>1.752700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>50</td>\n",
       "      <td>1.453900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>60</td>\n",
       "      <td>1.293000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>70</td>\n",
       "      <td>1.190300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>80</td>\n",
       "      <td>1.107200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>90</td>\n",
       "      <td>1.056200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.972600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>110</td>\n",
       "      <td>1.018500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>120</td>\n",
       "      <td>0.941100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>130</td>\n",
       "      <td>0.960800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>140</td>\n",
       "      <td>0.908000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>150</td>\n",
       "      <td>0.887800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>160</td>\n",
       "      <td>0.922400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>170</td>\n",
       "      <td>0.910400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>180</td>\n",
       "      <td>0.874500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>190</td>\n",
       "      <td>0.881900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>200</td>\n",
       "      <td>0.841800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>210</td>\n",
       "      <td>0.859900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>220</td>\n",
       "      <td>0.832600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>230</td>\n",
       "      <td>0.865100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>240</td>\n",
       "      <td>0.848300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>250</td>\n",
       "      <td>0.840100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>260</td>\n",
       "      <td>0.803200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>270</td>\n",
       "      <td>0.843100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>280</td>\n",
       "      <td>0.824200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>290</td>\n",
       "      <td>0.785000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>300</td>\n",
       "      <td>0.828800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>310</td>\n",
       "      <td>0.812400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>320</td>\n",
       "      <td>0.800100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>330</td>\n",
       "      <td>0.782000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>340</td>\n",
       "      <td>0.775800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>350</td>\n",
       "      <td>0.798300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>360</td>\n",
       "      <td>0.798200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>370</td>\n",
       "      <td>0.766800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>380</td>\n",
       "      <td>0.779700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>390</td>\n",
       "      <td>0.776600</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=390, training_loss=1.1107213252629988, metrics={'train_runtime': 447.539, 'train_samples_per_second': 13.787, 'train_steps_per_second': 0.871, 'total_flos': 6578583030988800.0, 'train_loss': 1.1107213252629988, 'epoch': 10.0})"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "5d05b294",
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen_prompt(tokenizer, sentence):\n",
    "    converted_sample = [{\"role\": \"user\", \"content\": sentence}]\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        converted_sample, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    return prompt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "c59e73b7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "The Force is strong in you!<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "sentence = 'The Force is strong in you!'\n",
    "prompt = gen_prompt(tokenizer, sentence)\n",
    "print(prompt)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "f3def236",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(model, tokenizer, prompt, max_new_tokens=64, skip_special_tokens=False):\n",
    "    tokenized_input = tokenizer(\n",
    "        prompt, add_special_tokens=False, return_tensors=\"pt\"\n",
    "    ).to(model.device)\n",
    "\n",
    "    model.eval()\n",
    "    gen_output = model.generate(**tokenized_input,\n",
    "                                eos_token_id=tokenizer.eos_token_id,\n",
    "                                max_new_tokens=max_new_tokens)\n",
    "    \n",
    "    output = tokenizer.batch_decode(gen_output, skip_special_tokens=skip_special_tokens)\n",
    "    return output[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9ed27d55",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are Qwen, created by Alibaba Cloud. You are a helpful assistant.<|im_end|>\n",
      "<|im_start|>user\n",
      "The Force is strong in you!<|im_end|>\n",
      "<|im_start|>assistant\n",
      "Strong in you! The Force is .<|im_end|>\n"
     ]
    }
   ],
   "source": [
    "print(generate(model, tokenizer, prompt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de6a6a1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
